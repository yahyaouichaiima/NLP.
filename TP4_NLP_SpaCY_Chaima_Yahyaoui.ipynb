{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yahyaouichaiima/NLP./blob/main/TP4_NLP_SpaCY_Chaima_Yahyaoui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP spacy"
      ],
      "metadata": {
        "id": "eZw7h5n3O2Mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08GCw0VJO5Q1",
        "outputId": "9639396a-56ce-4184-94c7-7a06678cac0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7d30bb483460>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nlp call"
      ],
      "metadata": {
        "id": "OEGR7ZAGPBu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "introduction_doc = nlp(\n",
        "...     \"This tutorial is about Natural Language Processing in spaCy.\"\n",
        "... )\n",
        "type(introduction_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZf4spnzPB6e",
        "outputId": "3c075241-aeaa-4c6a-ec5f-1fdf718f4270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenisation avec nlp"
      ],
      "metadata": {
        "id": "KaBpES5sPP5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[token.text for token in introduction_doc]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajf59QnLPQB9",
        "outputId": "50d39ebc-47c8-42ed-cf18-92335c30dc00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'tutorial',\n",
              " 'is',\n",
              " 'about',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " 'in',\n",
              " 'spaCy',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenisation from file"
      ],
      "metadata": {
        "id": "eTzz4_m9QBIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "file_name = \"/content/introduction.txt\"\n",
        "introduction_doc = nlp(pathlib.Path(file_name).read_text(encoding=\"utf-8\"))\n",
        "print ([token.text for token in introduction_doc])"
      ],
      "metadata": {
        "id": "gb8uyI_xQH4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15e06c57-cbe8-4d1e-c56e-97e93d6b65f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['English', 'texts', 'for', 'beginners', 'to', 'practice', 'reading', 'and', 'comprehension', 'online', 'and', 'for', 'free', '.', 'Practicing', 'your', 'comprehension', 'of', 'written', 'English', 'will', 'both', 'improve', 'your', 'vocabulary', 'and', 'understanding', 'of', 'grammar', 'and', 'word', 'order', '.', 'The', 'texts', 'below', 'are', 'designed', 'to', 'help', 'you', 'develop', 'while', 'giving', 'you', 'an', 'instant', 'evaluation', 'of', 'your', 'progress', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence detection"
      ],
      "metadata": {
        "id": "CfZGd346QTom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "about_text = (\n",
        "...     \"Gus Proto is a Python developer currently\"\n",
        "...     \" working for a London-based Fintech\"\n",
        "...     \" company. He is interested in learning\"\n",
        "...     \" Natural Language Processing.\"\n",
        "... )\n",
        "about_doc = nlp(about_text)\n",
        "sentences = list(about_doc.sents)\n",
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LY8pXVbQZeV",
        "outputId": "eda84613-45e7-484d-e114-f47a0ebb37ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "stemming"
      ],
      "metadata": {
        "id": "C4eMxtkHYuns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentences:\n",
        "   print(f\"{sentence[:5]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ-UtFH6Qdy8",
        "outputId": "5e729c48-0e53-47e5-c15d-50ccda933fde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gus Proto is a Python...\n",
            "He is interested in learning...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *"
      ],
      "metadata": {
        "id": "sk9I8B7nX3OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "qlhMXmsaX3Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['beginners','reading','Practicing','written','understanding']"
      ],
      "metadata": {
        "id": "CjfiSduBX-g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word+' --> '+p_stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHZ95sLTX-dQ",
        "outputId": "e4fb1ce7-b461-4dce-c779-afa58adac2d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beginners --> beginn\n",
            "reading --> read\n",
            "Practicing --> practic\n",
            "written --> written\n",
            "understanding --> understand\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lemmatization"
      ],
      "metadata": {
        "id": "VHeToH-jZPXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1FtwbZLcNm_",
        "outputId": "81069969-e6d2-4a3b-ad39-3ccae100d01f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_text(introduction_doc):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(introduction_doc)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "introduction_doc = \"Your introduction document here\"\n",
        "lemmatized_text = lemmatize_text(introduction_doc)\n",
        "print(lemmatized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_lKtCrndICb",
        "outputId": "b993cda3-6252-4e7e-9238-c05f33a15442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your introduction document here\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "POSTAG"
      ],
      "metadata": {
        "id": "p2EkPXCxZpZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Charger le modèle linguistique en anglais de SpaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def pos_tagging(text):\n",
        "    # Traiter le texte avec le modèle linguistique\n",
        "    doc = nlp(text)\n",
        "    # Extraire les étiquettes POS pour chaque token\n",
        "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "    return pos_tags\n",
        "\n",
        "\n",
        "pos_tags = pos_tagging(introduction_doc)\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeMBUIJNZpAh",
        "outputId": "00a24d0d-3fbd-4688-dda9-518f21a172f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('English', 'ADJ'), ('texts', 'NOUN'), ('for', 'ADP'), ('beginners', 'NOUN'), ('to', 'PART'), ('practice', 'VERB'), ('reading', 'NOUN'), ('and', 'CCONJ'), ('comprehension', 'NOUN'), ('online', 'ADV'), ('and', 'CCONJ'), ('for', 'ADP'), ('free', 'ADJ'), ('.', 'PUNCT'), ('Practicing', 'VERB'), ('your', 'PRON'), ('comprehension', 'NOUN'), ('of', 'ADP'), ('written', 'VERB'), ('English', 'PROPN'), ('will', 'AUX'), ('both', 'ADV'), ('improve', 'VERB'), ('your', 'PRON'), ('vocabulary', 'NOUN'), ('and', 'CCONJ'), ('understanding', 'NOUN'), ('of', 'ADP'), ('grammar', 'NOUN'), ('and', 'CCONJ'), ('word', 'NOUN'), ('order', 'NOUN'), ('.', 'PUNCT'), ('The', 'DET'), ('texts', 'NOUN'), ('below', 'ADV'), ('are', 'AUX'), ('designed', 'VERB'), ('to', 'PART'), ('help', 'VERB'), ('you', 'PRON'), ('develop', 'VERB'), ('while', 'SCONJ'), ('giving', 'VERB'), ('you', 'PRON'), ('an', 'DET'), ('instant', 'ADJ'), ('evaluation', 'NOUN'), ('of', 'ADP'), ('your', 'PRON'), ('progress', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NER"
      ],
      "metadata": {
        "id": "5VFgZGSLaxvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Charger le modèle linguistique en anglais de SpaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def ner_extraction(text):\n",
        "    # Traiter le texte avec le modèle linguistique\n",
        "    doc = nlp(text)\n",
        "    # Extraire les entités nommées\n",
        "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "    return entities\n",
        "\n",
        "\n",
        "entities = ner_extraction(introduction_doc)\n",
        "print(entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsJP132wayJ_",
        "outputId": "d9c97d6d-0ae3-4abf-bbed-3623a133c8b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('English', 'LANGUAGE'), ('English', 'LANGUAGE')]\n"
          ]
        }
      ]
    }
  ]
}